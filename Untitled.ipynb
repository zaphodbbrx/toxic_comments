{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = pd.read_csv('../train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "#comments = pd.read_csv('comments_en_cleaned.csv')\n",
    "#comments_en = comments[comments.lang == 'en']\n",
    "vect = CountVectorizer(ngram_range = (1,1), analyzer = 'word',\n",
    "                       stop_words = 'english',\n",
    "                       #max_features = 500,\n",
    "                       min_df = 10, max_df = 0.95).fit(td.comment_text)\n",
    "pw = list(vect.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pw.txt\", \"w\") as text_file:\n",
    "    for w in pw:\n",
    "        print(w, file=text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enchant\n",
    "c = enchant.Dict(\"en_UK\")\n",
    "def check_spelling(text):\n",
    "    if not c.check(text):\n",
    "        suggestions = list(set(c.suggest(text)).intersection(set(pw)))\n",
    "        if len(suggestions)>0:\n",
    "            res = suggestions[0]\n",
    "        elif len(c.suggest(text))>0:\n",
    "            res = c.suggest(text)[0]\n",
    "        else:\n",
    "            res = text\n",
    "    else:\n",
    "        res = text\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize,TreebankWordTokenizer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from emoji.unicode_codes import UNICODE_EMOJI\n",
    "import emoji\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def clean_comment(text):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    #deacc = re.sub(r'\\!',' exclamation_point ', text)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    processed = [wnl.lemmatize(check_spelling(word),get_wordnet_pos(tag)) if get_wordnet_pos(tag)!='' else wnl.lemmatize(check_spelling(word)) for (word, tag) in tags]\n",
    "#    for (word, tag) in tags:\n",
    "#        wn_tag = get_wordnet_pos(tag)\n",
    "#        if wn_tag!='':\n",
    "#            processed.append(wnl.lemmatize(check_spelling(word),wn_tag))\n",
    "#        else:\n",
    "#            processed.append(wnl.lemmatize(check_spelling(word)))\n",
    "    res = ' '.join(processed)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'u suck cock'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_comment('u suck coks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc = td.sample(frac = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = tdc.comment_text.apply(clean_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier,RidgeClassifierCV,LogisticRegressionCV, SGDClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "vect = CountVectorizer()\n",
    "model = LogisticRegressionCV()\n",
    "scaler = MaxAbsScaler()\n",
    "lin_model = Pipeline([('vectorizer', vect),\n",
    "                      #('features', FeatureUnion([\n",
    "                      #  ('ngram_tf_idf', Pipeline([\n",
    "                      #    ('counts', CountVectorizer()),\n",
    "                      #    ('tf_idf', TfidfTransformer())]))\n",
    "                      #])),\n",
    "                        #('scaler', scaler),\n",
    "                      ('classifier', model)])\n",
    "lin_model.set_params(vectorizer__ngram_range = (1,3),vectorizer__analyzer = 'word', vectorizer__stop_words = 'english',\n",
    "                     vectorizer__max_features = 800,\n",
    "                     vectorizer__min_df = 2, vectorizer__max_df = 0.95,\n",
    "                     vectorizer__vocabulary = vocab,\n",
    "                     \n",
    "                     classifier__class_weight = 'balanced')#,classifier__alphas = np.linspace(start = 0.01, stop = 100, num = 50))\n",
    "feats = labeled.cleaned\n",
    "labels = labeled.label_num\n",
    "lin_model = lin_model.fit(feats, labels)\n",
    "print('cross_val_score: %1.4f'% (np.mean(cross_val_score(lin_model, labeled.cleaned,labeled.label_num))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
