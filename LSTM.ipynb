{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "td = pd.read_csv('../train_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Explanation Why the edits make under my surnam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Dew ! He match this background colour I rm see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Hey man , I rm really not try to edit tar It c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>W More I ca mt make any real suggestion on imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>You , sir , be my hire Any chance you remember...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                id  \\\n",
       "0           0  0000997932d777bf   \n",
       "1           1  000103f0d9cfb60f   \n",
       "2           2  000113f07ec002fd   \n",
       "3           3  0001b41b1c6bb37e   \n",
       "4           4  0001d958c54c6e35   \n",
       "\n",
       "                                        comment_text  toxic  severe_toxic  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...      0             0   \n",
       "1  D'aww! He matches this background colour I'm s...      0             0   \n",
       "2  Hey man, I'm really not trying to edit war. It...      0             0   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0             0   \n",
       "4  You, sir, are my hero. Any chance you remember...      0             0   \n",
       "\n",
       "   obscene  threat  insult  identity_hate  \\\n",
       "0        0       0       0              0   \n",
       "1        0       0       0              0   \n",
       "2        0       0       0              0   \n",
       "3        0       0       0              0   \n",
       "4        0       0       0              0   \n",
       "\n",
       "                                             cleaned  \n",
       "0  Explanation Why the edits make under my surnam...  \n",
       "1  Dew ! He match this background colour I rm see...  \n",
       "2  Hey man , I rm really not try to edit tar It c...  \n",
       "3  W More I ca mt make any real suggestion on imp...  \n",
       "4  You , sir , be my hire Any chance you remember...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('compressed_w2v.pkl', 'rb') as f:\n",
    "    compressed_vectors = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "#comments = pd.read_csv('comments_en_cleaned.csv')\n",
    "#comments_en = comments[comments.lang == 'en']\n",
    "vect = CountVectorizer(ngram_range = (1,1), analyzer = 'word',\n",
    "                       stop_words = 'english',\n",
    "                       #max_features = 500,\n",
    "                       min_df = 10, max_df = 0.95).fit(td.comment_text)\n",
    "pw = list(vect.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"pw.txt\", \"w\") as text_file:\n",
    "    for w in pw:\n",
    "        print(w, file=text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import enchant\n",
    "c = enchant.Dict(\"en_UK\")\n",
    "def check_spelling(text):\n",
    "    if not c.check(text):\n",
    "        suggestions = list(set(c.suggest(text)).intersection(set(pw)))\n",
    "        if len(suggestions)>0:\n",
    "            res = suggestions[0]\n",
    "        elif len(c.suggest(text))>0:\n",
    "            res = c.suggest(text)[0]\n",
    "        else:\n",
    "            res = text\n",
    "    else:\n",
    "        res = text\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize,TreebankWordTokenizer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from emoji.unicode_codes import UNICODE_EMOJI\n",
    "import emoji\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def clean_comment(text):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    #deacc = re.sub(r'\\!',' exclamation_point ', text)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    processed = [wnl.lemmatize(check_spelling(word),get_wordnet_pos(tag)) if get_wordnet_pos(tag)!='' else wnl.lemmatize(check_spelling(word)) for (word, tag) in tags]\n",
    "#    for (word, tag) in tags:\n",
    "#        wn_tag = get_wordnet_pos(tag)\n",
    "#        if wn_tag!='':\n",
    "#            processed.append(wnl.lemmatize(check_spelling(word),wn_tag))\n",
    "#        else:\n",
    "#            processed.append(wnl.lemmatize(check_spelling(word)))\n",
    "    res = ' '.join(processed)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ti ski use'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_comment('thi si osome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tdc = td.sample(frac = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lsm/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "#from keras.layers.cudnn_recurrent import CuDNNLSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import TimeDistributed, Flatten\n",
    "from keras.models import load_model\n",
    "from keras.utils import Sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import keras.backend as K\n",
    "import gensim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import regularizers\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_padded_sequenses(docs, max_length):\n",
    "    tokens = [doc.split(' ') for doc in docs]\n",
    "    vecs = [[compressed_vectors[t] if t in compressed_vectors else np.zeros(30) for t in ts] for ts in tokens]\n",
    "    seqs = np.array([np.pad(np.vstack(v),mode = 'constant', pad_width = ((0,max_length-len(v)),(0,0))) if len(v)<max_length else np.vstack(v)[:max_length,:] for v in vecs])\n",
    "    return seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seqs = make_padded_sequenses(td.cleaned.tolist(),75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feats = seqs\n",
    "labels = td.toxic.as_matrix()\n",
    "X_train, X_test, y_train, y_test = train_test_split(feats, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_weight = compute_class_weight('balanced'\n",
    "                                               ,[0,1]\n",
    "                                               ,td.toxic.apply(int).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters=10, kernel_size=2, input_shape = seqs[0].shape, padding='same', activation='tanh'))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Conv1D(filters=20, kernel_size=3, padding='same', activation='tanh'))\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/10\n",
      "127656/127656 [==============================] - 9s 70us/step - loss: 0.1784 - acc: 0.9343 - val_loss: 0.1782 - val_acc: 0.9341\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.17822, saving model to weights.hdf5\n",
      "Epoch 2/10\n",
      "127656/127656 [==============================] - 9s 67us/step - loss: 0.1759 - acc: 0.9356 - val_loss: 0.1760 - val_acc: 0.9347\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.17822 to 0.17601, saving model to weights.hdf5\n",
      "Epoch 3/10\n",
      "127656/127656 [==============================] - 9s 70us/step - loss: 0.1736 - acc: 0.9366 - val_loss: 0.1787 - val_acc: 0.9344\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/10\n",
      "127656/127656 [==============================] - 9s 68us/step - loss: 0.1710 - acc: 0.9373 - val_loss: 0.1748 - val_acc: 0.9357\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.17601 to 0.17478, saving model to weights.hdf5\n",
      "Epoch 5/10\n",
      "127656/127656 [==============================] - 9s 71us/step - loss: 0.1693 - acc: 0.9380 - val_loss: 0.1707 - val_acc: 0.9381\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.17478 to 0.17069, saving model to weights.hdf5\n",
      "Epoch 6/10\n",
      "127656/127656 [==============================] - 9s 69us/step - loss: 0.1674 - acc: 0.9389 - val_loss: 0.1704 - val_acc: 0.9376\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.17069 to 0.17044, saving model to weights.hdf5\n",
      "Epoch 7/10\n",
      "127656/127656 [==============================] - 9s 70us/step - loss: 0.1660 - acc: 0.9394 - val_loss: 0.1690 - val_acc: 0.9389\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.17044 to 0.16896, saving model to weights.hdf5\n",
      "Epoch 8/10\n",
      "127656/127656 [==============================] - 9s 70us/step - loss: 0.1650 - acc: 0.9395 - val_loss: 0.1702 - val_acc: 0.9396\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/10\n",
      "127656/127656 [==============================] - 9s 69us/step - loss: 0.1638 - acc: 0.9405 - val_loss: 0.1664 - val_acc: 0.9395\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.16896 to 0.16642, saving model to weights.hdf5\n",
      "Epoch 10/10\n",
      "127656/127656 [==============================] - 9s 70us/step - loss: 0.1633 - acc: 0.9407 - val_loss: 0.1665 - val_acc: 0.9397\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f49a15b7828>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='weights.hdf5',\n",
    "                               verbose=1, save_best_only=True,\n",
    "                               monitor = 'val_loss')\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=500,\n",
    "          validation_data = [X_test,y_test], callbacks=[checkpointer],\n",
    "         class_weight = class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 10, 20)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
